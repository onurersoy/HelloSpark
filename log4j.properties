# 2.0: Log4J works almost the same way as Python logs (Python log is not integrated with Spark)
# Log4J has 3 components:
    # 1. Logger: Set of APIs which we are going to use from our application
    # 2. Configurations: Defined in the log4j.properties file; loaded by Loger on the run time. Defined in a hierarchy.
    # 3. Appender: Output destinations such as console and log files; these are defined in the log4j.properties file

########################## ROOT LEVEL LOG4J CONFIGURATION ##########################
# Set everything to be logged to the console (log level, list of appender)
# From the least severe to the most one: "ALL < DEBUG < INFO < WARN < ERROR < FATAL < OFF"
log4j.rootCategory=WARN, console
# The above will show us only warn and error messages

# Define console appender (this section is mostly the same on every application)
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.out
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
####################################################################################

########################## APPLICATION LEVEL LOG4J CONFIGURATION ###################
# Application log (we want a different behave for our application)
# 2nd log level that is specific for our application - Application level of logs
log4j.logger.guru.learningjournal.spark.examples=INFO, console, file
log4j.additivity.guru.learningjournal.spark.examples=false
# Application loger name above: "guru.learningjournal.spark.example"

# We already defined the console appender; so let's define the file appender
# Define rolling file appender
log4j.appender.file=org.apache.log4j.RollingFileAppender
log4j.appender.file.File=${spark.yarn.app.container.log.dir}/${logfile.name}.log
# 1st variable above: "${spark.yarn.app.container.log.dir}" >> Log file directory location
# 2nd variable above: "${logfile.name}.log" >> Log file name
#Define following in Java System
# -Dlog4j.configuration=file:log4j.properties
# -Dlogfile.name=hello-spark
# -Dspark.yarn.app.container.log.dir=app-logs
log4j.appender.file.ImmediateFlush=true
log4j.appender.file.Append=false
log4j.appender.file.MaxFileSize=500MB
log4j.appender.file.MaxBackupIndex=2
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.conversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
####################################################################################

# Recommendations from Spark template
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain#exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR